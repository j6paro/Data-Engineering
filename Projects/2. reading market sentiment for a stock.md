# 1. ê°œìš”
\- ì„œë¸Œë ˆë”§ wallstreetbets ì—ì„œ íŠ¹ì • ì¢…ëª©ì„ ê²€ìƒ‰í–ˆì„ ë•Œ ê²€ìƒ‰ë˜ëŠ” ê¸€ë“¤ì˜ ëŒ“ê¸€ë“¤ì„ ìŠ¤í¬ë©í•œ ë’¤ ê°ì„±ë¶„ì„ í•˜ì—¬ ë¯¼ì‹¬ì„ ì‚´í´ë³¸ë‹¤.  
\- ê¸ì •, ë¶€ì •, ì¤‘ë¦½ì˜ ë¹„ìœ¨ì„ ì§‘ê³„í•˜ê³  ì‹œê°í™”í•œë‹¤.  
\- ìƒì„±í˜• AI êµìœ¡ ìˆ˜ê°• ì‹œ ì§„í–‰í•œ í”„ë¡œì íŠ¸  

# 2. ë°ì´í„° íŒŒì´í”„ë¼ì¸
![image](https://github.com/user-attachments/assets/b261cee3-60e3-43eb-b169-3915a9bcc553)
  
  
1\) ë°ì´í„° ì¶”ì¶œ : íŒŒì´ì¬ PRAWë¥¼ ì´ìš©í•´ ë ˆë”§ - ì„œë¸Œë ˆë”§ wallstreetbets ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í–ˆì„ ë•Œ ë‚˜ì˜¤ëŠ” ê¸€ë“¤ì˜ ëŒ“ê¸€ë“¤ì„ ìŠ¤í¬ë©í•œë’¤ ì €ì¥í•œë‹¤.  
2\) Fine-Tuning  
\- ê°ì„±ë¶„ì„ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ë†’ì—¬ì£¼ê¸° ìœ„í•´ Fine-Tuningí•˜ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤.  
\- ëŒ“ê¸€ë“¤ì„ ë¶€ì •, ì¤‘ë¦½, ê¸ì • 3 ì¢…ë¥˜ë¡œ ë¼ë²¨ë§í•œ ë’¤ì— BERT, FinancialBERT ëª¨ë¸ì— í•™ìŠµì‹œí‚¨ë‹¤.  
\- í•™ìŠµì‹œí‚¨ ëª¨ë¸ë“¤ ì¤‘ ì •í™•ë„ê°€ ê°€ì¥ ë†’ì€ ëª¨ë¸ì€ ë”°ë¡œ ì €ì¥í•œë‹¤.  
3\) Streamlitì„ ì‚¬ìš©í•´ ì§‘ê³„í•œ ë°ì´í„°ë¥¼ ì‹œê°í™”í•œë‹¤. ìŠ¤í¬ë©í•œ ëŒ“ê¸€ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê¸°ëŠ¥ë„ ì¶”ê°€í–ˆë‹¤.  
\- ì„¤ì •í•œ ê¸°ê°„ ë™ì•ˆ íŠ¹ì • ì¢…ëª©ì— ëŒ€í•´ ëˆ„ì ëœ ê¸ì •, ì¤‘ë¦½, ë¶€ì • ë¹„ìœ¨  
\- ì„¤ì •í•œ ê¸°ê°„ ë™ì•ˆ ê°ê°ì˜ ë‚ ë“¤ì˜ ê¸ì •, ì¤‘ë¦½, ë¶€ì • ë¹„ìœ¨  
\- ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ë“¤ì˜ wordcloud  
\- ê°€ì¥ ë§ì€ ì¶”ì²œìˆ˜ë¥¼ ë°›ì€ ëŒ“ê¸€  
\- ìŠ¤í¬ë©í•œ ëŒ“ê¸€ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥  

# 3. ì½”ë“œ

### 3-1. ì‚¬ìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬  
1\) praw : ë ˆë”§ API. ë ˆë”§ ëŒ“ê¸€ë“¤ ìŠ¤í¬ë©ì— ì‚¬ìš©  
2\) torch : ëª¨ë¸ í•™ìŠµ, í‰ê°€ ì‹œ ì‚¬ìš©  
3\) transformers : BertTokenizer, BertForSequenceClassification ë¥¼ import í•˜ê¸°  
4\) matplotlib.pyplot : ì§‘ê³„í•œ ê²°ê³¼ ì‹œê°í™”  
5\) pandas : ìŠ¤í¬ë©í•œ ë°ì´í„° ì „ì²˜ë¦¬ ë“± ë°ì´í„°í”„ë ˆì„ ë‹¤ë£¨ê¸° ìœ„í•´  
6\) datetime, pytz : ì‹œê³„ì—´ë°ì´í„° ë‹¤ë£¨ê¸° ìœ„í•´  
7\_ streamlit : ìŠ¤í¬ë©í•œ ê²°ê³¼ ì§‘ê³„í•˜ì—¬ ì‹œê°í™”í•˜ê¸° ìœ„í•´  

### 3-1. ë ˆë”§ ëŒ“ê¸€ ìŠ¤í¬ë©  
```
import praw
import pandas as pd
from datetime import datetime, timedelta
import pytz

DIR_SAVE = "C:/Users/User/Desktop/code/project/savefile.csv" # ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ

# Reddit API ì¸ì¦ (ë³¸ì¸ì˜ í‚¤ë¡œ ëŒ€ì²´ í•„ìš”)
reddit = praw.Reddit(
    client_id='í´ë¼ì´ì–¸íŠ¸ ID', # ë ˆë”§ ê°œë°œì ê³„ì •ì˜ client ID
    client_secret='í´ë¼ì´ì–¸íŠ¸ Secret', # ë ˆë”§ ê°œë°œì ê³„ì •ì˜ secret code
    user_agent='ìœ ì € ë‹‰ë„¤ì„' # ë ˆë”§ ê°œë°œì ê³„ì •ì˜ ë‹‰ë„¤ì„ ì •ë³´
)

# íŠ¹ì • í‹°ì»¤ì— ëŒ€í•´ Reddit ì½”ë©˜íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ê³ , ìµœê·¼ nì¼ ì´ë‚´ì˜ ê²ƒë§Œ ë°˜í™˜
def scrape_reddit_comments(ticker, subreddit_name='wallstreetbets', days=1, limit=1000):
    utc = pytz.UTC
    end_date = datetime.now(utc)
    start_date = end_date - timedelta(days=days)

    comments_data = []

    # Subredditì—ì„œ ì¸ê¸° ê¸€ ê²€ìƒ‰
    subreddit = reddit.subreddit(subreddit_name)
    for submission in subreddit.search(ticker, sort='new', time_filter='week', limit=limit):
        submission.comments.replace_more(limit=0)  # ë” ë§ì€ ëŒ“ê¸€ ë¡œë”©
        for comment in submission.comments.list():
            comment_time = datetime.fromtimestamp(comment.created_utc, tz=utc)
            if start_date <= comment_time <= end_date:
                comments_data.append({
                    'comment_id': comment.id,
                    'created_utc': comment_time,
                    'body': comment.body,
                    'score': comment.score, # ì½”ë©˜íŠ¸ ì¶”ì²œìˆ˜
                    'submission_title': submission.title,
                    'permalink': f"https://reddit.com{comment.permalink}"
                })

    df = pd.DataFrame(comments_data)
    print(f"ì´ {len(df)}ê°œì˜ ëŒ“ê¸€ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return df

# ì €ì¥
ticker = "NVDA"  # ì˜ˆ: NVIDIA
comments_df = scrape_reddit_comments(ticker, days=3, limit=300) # ìˆ˜ì§‘í•œ ëŒ“ê¸€
print(comments_df.head()) # ë””ë²„ê¹…ìš©

comments_df.to_csv(DIR_SAVE)
```

\- 

### 3-2. ëª¨ë¸ Fine-Tuning
```
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from torch.utils.data import Dataset
import torch
import pandas as pd
import numpy as np

##################
# 1. ëª¨ë¸ íŒŒì¸íŠœë‹
##################

DIR_TRAIN = 'C:/Users/User/Desktop/code/project/train_data_nvda.csv' # í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ê²½ë¡œ

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv(DIR_TRAIN, encoding='latin1')  # comment, label

# 2. í† í¬ë‚˜ì´ì €, ëª¨ë¸, ë° ë°ì´í„°ì…‹
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # í† í¬ë‚˜ì´ì €
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3) # num_labels = 3 : ë¶€ì •, ì¤‘ë¦½, ê¸ì •

class RedditDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=100) # ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´ 100 í† í°
        self.labels = labels

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} | {'labels': torch.tensor(self.labels[idx])}

    def __len__(self):
        return len(self.labels)

train_texts, test_texts, train_labels, test_labels = train_test_split(df['body'].tolist(), df['label'].tolist(), test_size=0.2) # ì „ì²´ì—ì„œ 20% í…ŒìŠ¤íŠ¸

train_dataset = RedditDataset(train_texts, train_labels) # í›ˆë ¨ ë°ì´í„°ì…‹
test_dataset = RedditDataset(test_texts, test_labels) # í‰ê°€ ë°ì´í„°ì…‹

# 3. Trainer ì„¤ì • - ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
DIR_BEST_MODEL = 'C:/Users/User/Desktop/code/project/saved_model' # í•™ìŠµëœ ëª¨ë¸ ì €ì¥í•  ê²½ë¡œ
DIR_LOG = 'C:/Users/User/Desktop/code/project/logs' # ë¡œê·¸ íŒŒì¼ ì €ì¥í•  ê²½ë¡œ

training_args = TrainingArguments(
    output_dir = DIR_BEST_MODEL, # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    eval_strategy = 'epoch', # ë§¤ epochë§ˆë‹¤ í‰ê°€
    save_strategy = 'epoch', # epoch ëë‚  ë•Œë§ˆë‹¤
    save_total_limit = 1, # ëª¨ë¸ì€ ìµœëŒ€ 1ê°œë§Œ ì €ì¥
    load_best_model_at_end = True, # í›ˆë ¨ ëë‚  ë•Œë§ˆë‹¤ ë² ìŠ¤íŠ¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    metric_for_best_model = 'accuracy', # ë² ìŠ¤íŠ¸ ëª¨ë¸ ê¸°ì¤€
    greater_is_better = True, # í‰ê°€ ê¸°ì¤€ì´ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
    per_device_train_batch_size = 4, # í•™ìŠµ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
    per_device_eval_batch_size = 8, # í‰ê°€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
    num_train_epochs = 4, # í›ˆë ¨ epoch ìˆ˜
    logging_dir = DIR_LOG, # ë¡œê·¸ ì €ì¥
    logging_steps = 10, # ë¡œê·¸ ì¶œë ¥ ê°„ê²© ìŠ¤í…
    use_cpu = True # CPU ì‚¬ìš© ì—¬ë¶€
)

# 4. ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜
def compute_metrics(predict):
    preds = np.argmax(predict.predictions, axis=1)
    acc = accuracy_score(predict.label_ids, preds)
    return {'accuracy': acc}

# 5. trainer
trainer = Trainer(
    model = model, # ì‚¬ìš©í•  ëª¨ë¸
    args = training_args, # í•™ìŠµ í™˜ê²½
    train_dataset = train_dataset, # í›ˆë ¨ ë°ì´í„°
    eval_dataset = test_dataset, # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    compute_metrics = compute_metrics, # í‰ê°€ ê¸°ì¤€
)

# 6. í•™ìŠµ ì‹¤í–‰
trainer.train()

# ìµœì¢… ëª¨ë¸ ì €ì¥
# ëª¨ë¸ ì €ì¥ ê²½ë¡œ
DIR_SAVED_MODEL = 'C:/Users/User/Desktop/code/project/saved_model/final_model'
trainer.save_model(DIR_SAVED_MODEL)
tokenizer.save_pretrained(DIR_SAVED_MODEL)
```

### 3.3. Fine-Tuningí•œ ëª¨ë¸ ì±„ì 
```
# ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì™€ì„œ í…ŒìŠ¤íŠ¸í•˜ê¸°

from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
import torch
import pandas as pd

# ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# ëª¨ë¸ ì €ì¥ ê²½ë¡œ
DIR_SAVED_MODEL = 'C:/Users/User/Desktop/code/project/saved_model/final_model'
model = BertForSequenceClassification.from_pretrained(DIR_SAVED_MODEL) # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = BertTokenizer.from_pretrained(DIR_SAVED_MODEL) # í† í¬ë‚˜ì´ì €
model.eval()

# ë‹¤ë¥¸ ì¢…ëª© í…ìŠ¤íŠ¸ ìë£Œ
DIR_TEST = 'C:/Users/User/Desktop/code/project/test_data_tsla_classified.csv'
df_tsla =pd.read_csv(DIR_TEST, encoding='latin1')  # comment, label

# ì „ì²´ ë°ì´í„°ë¥¼ í…ŒìŠ¤íŠ¸ë¡œ ì‚¬ìš©
texts = df_tsla['body'].tolist()
true_labels = df_tsla['label'].tolist()

# ë°°ì¹˜ ì²˜ë¦¬ ì—†ì´ í•œ ë²ˆì— ì¶”ë¡  (ë°ì´í„°ê°€ ì ë‹¤ê³  ê°€ì •)
inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=100)

# ì¶”ë¡ 
with torch.no_grad():
    outputs = model(**inputs)

# ì˜ˆì¸¡ê°’ ì¶”ì¶œ
preds = torch.argmax(outputs.logits, dim=1).tolist()

# ê²°ê³¼ DataFrame ìƒì„±
df_result = pd.DataFrame({
    'comment': texts,
    'label': true_labels,
    'predicted_label': preds
})

# ê²°ê³¼ ì¶œë ¥
print(df_result)
DIR_CHECK_ANOTHER = 'C:/Users/User/Desktop/code/project/check_tsla.csv'
df_result.to_csv(DIR_CHECK_ANOTHER)
```
\- ë³´ë‹¤ ì •í™•ë„ê°€ ë†’ì€ ëª¨ë¸ì„ ì„ ì •í•˜ê¸° ìœ„í•œ ì¶”ê°€ ì±„ì  ê³¼ì •  
\- í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì—”ë¹„ë””ì•„(NVDA) ì¢…ëª©ì— ê´€í•œ ê¸€ë“¤ì˜ ëŒ“ê¸€ë“¤ì„ ì‚¬ìš©í–ˆê³ , ì¶”ê°€ ì±„ì ì—ëŠ” í…ŒìŠ¬ë¼(TSLA) ì¢…ëª©ì— ê´€í•œ ê¸€ë“¤ì˜ ëŒ“ê¸€ë“¤ì„ ì‚¬ìš©í–ˆë‹¤.  


![image](https://github.com/user-attachments/assets/c1b29b3c-5e5a-4a57-ac0c-e09237639580)

\- ë‚˜ê°™ì€ ê²½ìš°ëŠ” 2ê°œì˜ hyper parameterë¥¼ ì¡°ì •í–ˆë‹¤ : epoch ìˆ˜, ë°ì´í„°ì…‹ ìˆ˜  
\- ì´ 24ë²ˆ í›ˆë ¨ì‹œì¼°ì§€ë§Œ fine-tuning, ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì •ë‹µë¥ ì˜ ìµœëŒ€ í‰ê· ì´ 60%ë¥¼ ë„˜ì§€ ëª»í–ˆë‹¤. ë” ë§ì€ ë°ì´í„°ì…‹ì„ ê°€ì§€ê³  í›ˆë ¨ì‹œì¼œì•¼ í•  ë“¯  


### 3-4. ì „ì²´ ì„œë¹„ìŠ¤ ì½”ë“œ
```
import streamlit as st
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime, timedelta
import pytz
import praw


##############################################
# 1. ë ˆë”§ ì½”ë©˜íŠ¸ ìŠ¤í¬ë©
##############################################

# Reddit API ì¸ì¦
reddit = praw.Reddit(
    client_id='í´ë¼ì´ì–¸íŠ¸ ID',
    client_secret='í´ë¼ì´ì–¸íŠ¸ Secret',
    user_agent='ìœ ì € ë‹‰ë„¤ì„'
)

# íŠ¹ì • í‹°ì»¤ì— ëŒ€í•´ Reddit ì½”ë©˜íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ê³ , ìµœê·¼ nì¼ ì´ë‚´ì˜ ê²ƒë§Œ ë°˜í™˜
def scrape_reddit_comments(ticker, subreddit_name='wallstreetbets', days=3, limit=1000):
    utc = pytz.UTC
    end_date = datetime.now(utc)
    start_date = end_date - timedelta(days=days)

    comments_data = []

    # Subredditì—ì„œ ì¸ê¸° ê¸€ ê²€ìƒ‰
    subreddit = reddit.subreddit(subreddit_name)
    for submission in subreddit.search(ticker, sort='new', time_filter='week', limit=limit):
        submission.comments.replace_more(limit=0)  # ë” ë§ì€ ëŒ“ê¸€ ë¡œë”©
        for comment in submission.comments.list():
            comment_time = datetime.fromtimestamp(comment.created_utc, tz=utc)
            if start_date <= comment_time <= end_date:
                comments_data.append({
                    'comment_id': comment.id,
                    'created_utc': comment_time,
                    'body': comment.body,
                    'score': comment.score, # ì½”ë©˜íŠ¸ ì¶”ì²œìˆ˜
                    'submission_title': submission.title,
                    'permalink': f"https://reddit.com{comment.permalink}"
                })

    df = pd.DataFrame(comments_data) # ìŠ¤í¬ë©í•œ ì½”ë©˜íŠ¸ ë‹´ê²¨ ìˆëŠ” ë°ì´í„°í”„ë ˆì„

    # date ì»¬ëŸ¼ ì¶”ê°€ : 'ë…„ë„-ì›”-ì¼' í˜•ì‹
    df['date'] = pd.to_datetime(df['created_utc']) # created_utcë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
    df['date'] = df['date'].dt.tz_convert('Asia/Seoul') # UTC -> KST(Asia/Seoul) ì„œìš¸ë¡œ ì‹œê°„ëŒ€ ë³€í™˜
    df['date'] = df['date'].dt.date # 'ë…„ë„-ì›”-ì¼' í˜•ì‹ì˜ ë‚ ì§œ ì •ë³´ë§Œ ì¶”ì¶œí•œ ìƒˆ ì»¬ëŸ¼

    return df

##############################################
# 2. ê¸ì •, ë¶€ì •, ì¤‘ë¦½ ë¶„ì„ ëª¨ë¸
##############################################

def load_model_and_tokenizer(model_path):
    model = BertForSequenceClassification.from_pretrained(model_path) # ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    tokenizer = BertTokenizer.from_pretrained(model_path) # ì €ì¥ëœ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
    return model, tokenizer

def predict_sentiments(model, tokenizer, df):
    model.eval() # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
    sentiments = []
    with torch.no_grad():
        for text in df["body"].tolist():
            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=100)
            outputs = model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1).item()
            sentiments.append(preds)

    df["label"] = sentiments # ê¸ì •, ë¶€ì •, ì¤‘ë¦½ íŒë³„í•´ì„œ ë¼ë²¨ë§í•˜ê¸°
    label_map = {0: "ë¶€ì •", 1: "ì¤‘ë¦½", 2: "ê¸ì •"}
    df["sentiment"] = df["label"].map(label_map) # ë¼ë²¨ë§ëœ íŒë³„ í•´ì„
    return df

##############################################
# 3. ì‹œê°í™”
##############################################

def show_daily_sentiment_chart(df):
    daily = df.groupby(["date", "sentiment"]).size().unstack().fillna(0)
    st.line_chart(daily)

def show_cumulative_chart(df):
    cumulative = df["sentiment"].value_counts()
    st.bar_chart(cumulative)

def show_wordcloud(df):

    text = " ".join(df["body"])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    st.pyplot(plt)

def get_top_comment(df): # ì¶”ì²œìˆ˜ê°€ ê°€ì¥ ë§ì€ ëŒ“ê¸€
    top = df.sort_values(by="score", ascending=False).iloc[0]
    return top["body"]

# ê¸°ëŠ¥ ì‚­ì œ
# def save_output_comment(df, ticker):
#     df.to_csv(f"outputs/{ticker}_result.csv", encoding = 'utf-8-sig', index=False)

#     with open(f"outputs/{ticker}_chatbot_response.txt", "w", encoding="utf-8") as f:
#         f.write("ê¸ë¶€ì • ê²°ê³¼ ìš”ì•½\n")
#         f.write(str(df["sentiment"].value_counts()))

##############################################
# 4. streamlit
##############################################

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title='ì¢…ëª© ë¯¼ì‹¬ ì‚´í”¼ê¸°', layout='wide')
st.markdown("ğŸ’¬ **ì›í•˜ëŠ” ì¢…ëª© í‹°ì»¤ì™€ ê¸°ê°„ì„ ì…ë ¥í•˜ë©´ Reddit ì½”ë©˜íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¯¼ì‹¬ ë¶„ì„ì„ í•´ë“œë ¤ìš”!**")

# ìƒíƒœ ì €ì¥ì„ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_contexts" not in st.session_state:
    st.session_state.last_contexts = []
if 'ticker_history' not in st.session_state:
    st.session_state.ticker_history = []
if "latest_ticker" not in st.session_state:
    st.session_state.latest_ticker = "" # í˜ì´ì§€ê°€ ìƒˆë¡œ ë Œë”ë§ ë  ë•Œë§ˆë‹¤ ì´ì „ì— ì¡°ì‚¬ëœ í‹°ì»¤ëª… ì¶œë ¥í•˜ê¸° ìœ„í•´

# ê¸°ì¡´ ëŒ€í™” ë‚´ìš© ì¶œë ¥
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì´ì „ ì¶œë ¥ëœ ê·¸ë˜í”„ ë„ì›Œë†“ê¸°
if "last_comments_df" in st.session_state:
    with st.chat_message("assistant"):
        st.markdown(f"ğŸ“… {st.session_state.latest_ticker} ë¶„ì„ ê²°ê³¼ - ì¼ìë³„ ë¯¼ì‹¬ ë¹„ìœ¨")
        show_daily_sentiment_chart(st.session_state.last_comments_df)
        st.markdown(f"ğŸ“Š {st.session_state.latest_ticker} ë¶„ì„ ê²°ê³¼ - ëˆ„ì  ë¯¼ì‹¬ ë¹„ìœ¨")
        show_cumulative_chart(st.session_state.last_comments_df)
        st.markdown(f"ğŸ’¡ {st.session_state.latest_ticker} WordCloud")
        show_wordcloud(st.session_state.last_comments_df)

# ì±„íŒ… ì…ë ¥ì°½
user_input = st.chat_input("ë¯¼ì‹¬ì´ ê¶ê¸ˆí•œ í‹°ì»¤ë¥¼ ê²€ìƒ‰í•´ë³´ì„¸ìš”. ê²€ìƒ‰ ì–‘ì‹ : í‹°ì»¤, ê¸°ê°„ (NVDA, 3)")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    
    try:
        ticker, days = str(user_input).strip().split(',') # ì…ë ¥ë°›ì€ ê°’ì„ í‹°ì»¤, ìŠ¤í¬ë© ê¸°ê°„ìœ¼ë¡œ ë³€ê²½
    except:
        bot_response14 = "ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”."
        with st.chat_message("assistant"):
            st.markdown(bot_response14)
        st.session_state.messages.append({"role": "assistant", "content": bot_response14})
    else:
        if ticker and days:
            st.session_state.latest_ticker = str(ticker).upper() # í˜ì´ì§€ê°€ ìƒˆë¡œ ë Œë”ë§ ë  ë•Œë§ˆë‹¤ ì´ì „ì— ì¡°ì‚¬ëœ í‹°ì»¤ëª… ì¶œë ¥í•˜ê¸° ìœ„í•´
            st.session_state.ticker_history.append(ticker.upper()) # ì…ë ¥ëœ í‹°ì»¤ ëª¨ë‘ ëŒ€ë¬¸ìë¡œ ë³€í™˜

            with st.spinner(f"{ticker.upper()}ì— ëŒ€í•œ ì½”ë©˜íŠ¸ ìˆ˜ì§‘ ì¤‘..."):
                comments_df = scrape_reddit_comments(ticker, days=int(days)) # ë ˆë”§ ì½”ë©˜íŠ¸ ìŠ¤í¬ë©
                comments_df.to_csv(f"data/{ticker.upper()}_comments.csv", index=False) # ìŠ¤í¬ë©ëœ ì½”ë©˜íŠ¸ë“¤ ì €ì¥

            bot_response1 = f"{ticker.upper()} ê´€ë ¨ ì´ {len(comments_df)}ê°œì˜ ì½”ë©˜íŠ¸ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤." # ë´‡ ë‹µë³€1. ìˆ˜ì§‘ ì™„ë£Œ ë¬¸êµ¬
            bot_response2 = f"{ticker.upper()} ì˜ ë¯¼ì‹¬ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤." # ë´‡ ë‹µë³€2. ê°ì„± ë¶„ì„ ì¤‘ ë¬¸êµ¬ ë„ìš°ê¸°
            with st.chat_message("assistant"): # ìˆ˜ì§‘ ì™„ë£Œ ë¬¸êµ¬, ê°ì„± ë¶„ì„ ì¤‘ ë¬¸êµ¬ ì¶œë ¥
                st.markdown(bot_response1)
                st.markdown(bot_response2)
            
            # ê°ì„± ë¶„ì„
            DIR_MODEL = 'C:/Users/User/Desktop/code/project/saved_model/final_model' # ë¶ˆëŸ¬ì˜¬ ëª¨ë¸ ê²½ë¡œ
            model, tokenizer = load_model_and_tokenizer(DIR_MODEL) # ì €ì¥ëœ ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
            comments_df = predict_sentiments(model, tokenizer, comments_df) # íŒë³„ ê²°ê³¼ ì»¬ëŸ¼ ì¶”ê°€í•˜ì—¬ ë°˜í™˜
            
            bot_response3 = f"{ticker.upper()} ì˜ ë¯¼ì‹¬ì´ ë¶„ì„ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë¦¬ì‹­ì‹œì˜¤." # ë´‡ ë‹µë³€3. ê°ì„± ë¶„ì„ ì™„ë£Œ ë¬¸êµ¬ ë„ìš°ê¸°

            with st.chat_message("assistant"): # ê°ì„± ë¶„ì„ ì™„ë£Œ ë¬¸êµ¬ ì¶œë ¥
                st.markdown(bot_response3)

            # ì‹œê°í™” ë° ì±—ë´‡ ë‹µë³€ ì¶œë ¥
            bot_response4 = f"ğŸ“… {ticker.upper()} ì¼ìë³„ ë¯¼ì‹¬ ë¹„ìœ¨" # ë´‡ ë‹µë³€4. ì¼ìë³„ ë¯¼ì‹¬ ë¹„ìœ¨ ë¬¸êµ¬
            bot_response5 = f"ğŸ“Š {ticker.upper()} ëˆ„ì  ë¯¼ì‹¬ ë¹„ìœ¨" # ë´‡ ë‹µë³€5. ëˆ„ì  ë¯¼ì‹¬ ë¹„ìœ¨ ë¬¸êµ¬
            bot_response6 = "ğŸ’¡ ë‹¨ì–´ Wordcloud" # ë´‡ ë‹µë³€6. wordcloud ë¬¸êµ¬
            bot_response7 = "ğŸ† ì¶”ì²œ ìˆ˜ ê°€ì¥ ë†’ì€ ì½”ë©˜íŠ¸" # ë´‡ ë‹µë³€7. ë² ìŠ¤íŠ¸ ì½”ë©˜íŠ¸ ë¬¸êµ¬
            bot_response8 = get_top_comment(comments_df) # ë´‡ ë‹µë³€8. ë² ìŠ¤íŠ¸ ì½”ë©˜íŠ¸
            bot_response9 = "âœ… ì½”ë©˜íŠ¸ ìë£Œ ë‹¤ìš´ë¡œë“œ" # ë´‡ ë‹µë³€9. ì½”ë©˜íŠ¸ ìë£Œ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            bot_response10 = "ğŸ” ë¶„ì„ì´ ëë‚¬ìŠµë‹ˆë‹¤! ë” ê¶ê¸ˆí•œ ì¢…ëª©ì´ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš” ğŸ˜Š" # ë´‡ ë‹µë³€10. ë§ˆë¬´ë¦¬ ì•ˆë‚´ ë©”ì‹œì§€

            # ì±—ë´‡ ëŒ€ë‹µ
            with st.chat_message("assistant"): # ì¼ìë³„, ëˆ„ì  ë¯¼ì‹¬, ë² ìŠ¤íŠ¸ ì½”ë©˜íŠ¸ ì¶œë ¥
                st.markdown(bot_response4)
                bot_response11 = show_daily_sentiment_chart(comments_df) # ì¼ìë³„ ë¯¼ì„
                st.markdown(bot_response5)
                bot_response12 = show_cumulative_chart(comments_df) # ëˆ„ì  ë¯¼ì‹¬ ì¶œë ¥
                st.markdown(bot_response6)
                bot_response13 = show_wordcloud(comments_df) # wordcloud ì¶œë ¥
                st.markdown(bot_response7)
                st.markdown(bot_response8)
                st.markdown(bot_response9)

            st.session_state.last_comments_df = comments_df # ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ì…ë ¥ í›„ì—ë„ ê·¸ë˜í”„ ì‚¬ë¼ì§€ì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•´

            csv = comments_df.to_csv(index=False).encode('utf-8-sig') # ë‹¤ìš´ë¡œë“œ ìœ„í•´ csvë¡œ ë³€í™˜
            st.download_button(
                label = "ì½”ë©˜íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data = csv,
                file_name = f"{ticker}_comments.csv",
                mime="text/csv"
            )
```

# 4. ê²°ê³¼ë¬¼

![image](https://github.com/user-attachments/assets/dc76b7c6-7f09-433d-97d9-930f4a403c31)

![image](https://github.com/user-attachments/assets/9edad9e5-df17-4dcb-ade7-f29125d37683)

![image](https://github.com/user-attachments/assets/8425a454-b05a-42e5-b19f-3c43f316bd54)

![image](https://github.com/user-attachments/assets/1d9c7fe3-b01c-4369-b8cb-b739d064358d)





