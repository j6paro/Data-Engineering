# 1. 개요
\- 서브레딧 dividends 에서 특정 키워드를 검색했을 때 검색되는 글들의 제목에서 단어들을 집계한다.

# 2. 데이터 파이프라인
![image](https://github.com/user-attachments/assets/fe754edf-582c-401d-81fd-62bf404d0149)

1\) 데이터 추출 : 파이썬 PRAW를 이용해 레딧 - 서브레딧 dividends에서 특정 키워드를 검색했을 때 나오는 글들을 스크랩해 MongoDB에 저장한다. (예시 키워드는 dividends)  
2\) 데이터 집계, 저장 : PySpark를 이용해 글들의 제목을 파싱하고 단어들을 집계한다. (여기선 단순히 count 한다.)  

# 3. 코드

### 3-1. 데이터 추출
```
import praw
import pymongo
from pymongo import MongoClient

# Reddit API 인증 정보
client_id = "user script"
client_secret = "secret code"
user_agent = "MyRedditApp:v1.0 (by /u/계정 닉네임)"

# MongoDB 설정
mongo_client = MongoClient("mongodb://localhost:27017/")  # 로컬 MongoDB에 연결
db = mongo_client["reddit_data"]  # 데이터베이스 생성 또는 선택
collection = db["portfolio_posts"]  # 컬렉션 생성 또는 선택

# Reddit API 인증
reddit = praw.Reddit(
    client_id=client_id, client_secret=client_secret, user_agent=user_agent
)

# r/dividends 서브레딧에서 "portfolio"로 검색
subreddit = reddit.subreddit("dividends")
search_query = "portfolio"
posts = subreddit.search(
    search_query, limit=1000, sort="new", time_filter="year"
)  # 최대 n개의 글 가져오기

# MongoDB에 데이터 저장
for post in posts:
    try:
        post_data = {
            "id": post.id,
            "title": post.title.encode("utf-8").decode(
                "utf-8", "ignore"
            ),  # UTF-8로 처리
            "selftext": post.selftext.encode("utf-8").decode("utf-8", "ignore"),
            "author": str(post.author),
            "score": post.score,
            "created_utc": post.created_utc,
            "num_comments": post.num_comments,
            "permalink": post.permalink,
        }
        collection.update_one({"id": post.id}, {"$set": post_data}, upsert=True)
    except Exception as e:
        print(f"Error processing post {post.id}: {e}")


print("Data collection completed and saved to MongoDB.")
```

### 3-2. 데이터 집계, csv 파일로 저장
```
# 상수 및 변수 등 모음
DIR_RESULT = "C:/coding/DA/result/reddit_dividends_portfolio"
mongo_client = MongoClient("mongodb://localhost:27017/")
db = mongo_client["reddit_data"]  # 데이터베이스 선택
collection = db["portfolio_posts"]  # 컬렉션 선택
delete_words = [집계에서 제외할 단어들]

### Spark 이용해서 MongoDB에서 읽어들이기 ###
# 1. SparkSession 생성
spark = (
    SparkSession.builder.appName("MongoDB-PySpark Integration")
    .config(
        "spark.mongodb.read.connection.uri",
        "mongodb://localhost:27017/reddit_data.portfolio_posts",
    )
    .config(
        "spark.mongodb.write.connection.uri",
        "mongodb://localhost:27017/reddit_data.portfolio_posts",
    )
    .config(
        "spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:10.4.0"
    )
    .getOrCreate()
)

# MongoDB에서 데이터 로드
df = spark.read.format("mongodb").load()

# 데이터 스키마 확인
df.printSchema()

# "created_utc"를 datetime으로 변환
df = df.withColumn("datetime", from_unixtime(col("created_utc")).cast("timestamp"))

# 주요 데이터 필드 선택
df_selected = df.select("title", "author", "score", "num_comments", "datetime")

# title 컬럼의 모든 문장을 단어별로 split
words_df = df.select(explode(split(col("title"), "\\s+")).alias("word"))

# 단어별 빈도 집계
word_count_df = words_df.groupBy("word").count().orderBy(col("count").desc())

# 불필요한 단어 필터링
word_count_df = word_count_df.filter(~col("word").isin(delete_words))

# 결과 출력
word_count_df.show(truncate=False)

word_count_df.write.csv(
    path=DIR_RESULT,  # 저장 경로
    mode="overwrite",  # 이미 파일이 있을 경우 덮어쓰기
    header=True,  # 첫 줄에 컬럼 이름 포함
)

print(f"csv 파일이 저장되었습니다.")  # 저장 완료 문구 출력
```
