# 1. 개요
\- 서브레딧 wallstreetbets 에서 특정 종목을 검색했을 때 검색되는 글들의 댓글들을 스크랩한 뒤 감성분석 하여 민심을 살펴본다.  
\- 긍정, 부정, 중립의 비율을 집계하고 시각화한다.  
\- 생성형 AI 교육 수강 시 진행한 프로젝트  

# 2. 데이터 파이프라인
![image](https://github.com/user-attachments/assets/b261cee3-60e3-43eb-b169-3915a9bcc553)
  
  
1\) 데이터 추출 : 파이썬 PRAW를 이용해 레딧 - 서브레딧 wallstreetbets 에서 특정 키워드를 검색했을 때 나오는 글들의 댓글들을 스크랩한뒤 저장한다.  
2\) Fine-Tuning  
\- 감성분석 모델의 정확도를 높여주기 위해 Fine-Tuning하는 과정을 거친다.  
\- 댓글들을 부정, 중립, 긍정 3 종류로 라벨링한 뒤에 BERT, FinancialBERT 모델에 학습시킨다.  
\- 학습시킨 모델들 중 정확도가 가장 높은 모델은 따로 저장한다.  
3\) Streamlit을 사용해 집계한 데이터를 시각화한다. 스크랩한 댓글을 다운로드하는 기능도 추가했다.  
\- 설정한 기간 동안 특정 종목에 대해 누적된 긍정, 중립, 부정 비율  
\- 설정한 기간 동안 각각의 날들의 긍정, 중립, 부정 비율  
\- 많이 사용된 단어들의 wordcloud  
\- 가장 많은 추천수를 받은 댓글  
\- 스크랩한 댓글 다운로드 기능  

# 3. 코드

### 3-1. 사용한 라이브러리  
1\) praw : 레딧 API. 레딧 댓글들 스크랩에 사용  
2\) torch : 모델 학습, 평가 시 사용  
3\) transformers : BertTokenizer, BertForSequenceClassification 를 import 하기  
4\) matplotlib.pyplot : 집계한 결과 시각화  
5\) pandas : 스크랩한 데이터 전처리 등 데이터프레임 다루기 위해  
6\) datetime, pytz : 시계열데이터 다루기 위해  
7\_ streamlit : 스크랩한 결과 집계하여 시각화하기 위해  

### 3-1. 레딧 댓글 스크랩  
```
import praw
import pandas as pd
from datetime import datetime, timedelta
import pytz

DIR_SAVE = "C:/Users/User/Desktop/code/project/savefile.csv" # 결과물 저장 경로

# Reddit API 인증 (본인의 키로 대체 필요)
reddit = praw.Reddit(
    client_id='클라이언트 ID', # 레딧 개발자 계정의 client ID
    client_secret='클라이언트 Secret', # 레딧 개발자 계정의 secret code
    user_agent='유저 닉네임' # 레딧 개발자 계정의 닉네임 정보
)

# 특정 티커에 대해 Reddit 코멘트를 수집하고, 최근 n일 이내의 것만 반환
def scrape_reddit_comments(ticker, subreddit_name='wallstreetbets', days=1, limit=1000):
    utc = pytz.UTC
    end_date = datetime.now(utc)
    start_date = end_date - timedelta(days=days)

    comments_data = []

    # Subreddit에서 인기 글 검색
    subreddit = reddit.subreddit(subreddit_name)
    for submission in subreddit.search(ticker, sort='new', time_filter='week', limit=limit):
        submission.comments.replace_more(limit=0)  # 더 많은 댓글 로딩
        for comment in submission.comments.list():
            comment_time = datetime.fromtimestamp(comment.created_utc, tz=utc)
            if start_date <= comment_time <= end_date:
                comments_data.append({
                    'comment_id': comment.id,
                    'created_utc': comment_time,
                    'body': comment.body,
                    'score': comment.score, # 코멘트 추천수
                    'submission_title': submission.title,
                    'permalink': f"https://reddit.com{comment.permalink}"
                })

    df = pd.DataFrame(comments_data)
    print(f"총 {len(df)}개의 댓글이 수집되었습니다.")
    return df

# 저장
ticker = "NVDA"  # 예: NVIDIA
comments_df = scrape_reddit_comments(ticker, days=3, limit=300) # 수집한 댓글
print(comments_df.head()) # 디버깅용

comments_df.to_csv(DIR_SAVE)
```

\- 

### 3-2. 모델 Fine-Tuning
```
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from torch.utils.data import Dataset
import torch
import pandas as pd
import numpy as np

##################
# 1. 모델 파인튜닝
##################

DIR_TRAIN = 'C:/Users/User/Desktop/code/project/train_data_nvda.csv' # 학습에 사용되는 데이터 경로

# 1. 데이터 불러오기
df = pd.read_csv(DIR_TRAIN, encoding='latin1')  # comment, label

# 2. 토크나이저, 모델, 및 데이터셋
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # 토크나이저
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3) # num_labels = 3 : 부정, 중립, 긍정

class RedditDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=100) # 문장 최대 길이 100 토큰
        self.labels = labels

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} | {'labels': torch.tensor(self.labels[idx])}

    def __len__(self):
        return len(self.labels)

train_texts, test_texts, train_labels, test_labels = train_test_split(df['body'].tolist(), df['label'].tolist(), test_size=0.2) # 전체에서 20% 테스트

train_dataset = RedditDataset(train_texts, train_labels) # 훈련 데이터셋
test_dataset = RedditDataset(test_texts, test_labels) # 평가 데이터셋

# 3. Trainer 설정 - 베스트 모델 저장
DIR_BEST_MODEL = 'C:/Users/User/Desktop/code/project/saved_model' # 학습된 모델 저장할 경로
DIR_LOG = 'C:/Users/User/Desktop/code/project/logs' # 로그 파일 저장할 경로

training_args = TrainingArguments(
    output_dir = DIR_BEST_MODEL, # 베스트 모델 저장 경로
    eval_strategy = 'epoch', # 매 epoch마다 평가
    save_strategy = 'epoch', # epoch 끝날 때마다
    save_total_limit = 1, # 모델은 최대 1개만 저장
    load_best_model_at_end = True, # 훈련 끝날 때마다 베스트 모델 불러오기
    metric_for_best_model = 'accuracy', # 베스트 모델 기준
    greater_is_better = True, # 평가 기준이 높을수록 좋음
    per_device_train_batch_size = 4, # 학습 배치 사이즈
    per_device_eval_batch_size = 8, # 평가 배치 사이즈
    num_train_epochs = 4, # 훈련 epoch 수
    logging_dir = DIR_LOG, # 로그 저장
    logging_steps = 10, # 로그 출력 간격 스텝
    use_cpu = True # CPU 사용 여부
)

# 4. 정확도 계산 함수
def compute_metrics(predict):
    preds = np.argmax(predict.predictions, axis=1)
    acc = accuracy_score(predict.label_ids, preds)
    return {'accuracy': acc}

# 5. trainer
trainer = Trainer(
    model = model, # 사용할 모델
    args = training_args, # 학습 환경
    train_dataset = train_dataset, # 훈련 데이터
    eval_dataset = test_dataset, # 테스트 데이터
    compute_metrics = compute_metrics, # 평가 기준
)

# 6. 학습 실행
trainer.train()

# 최종 모델 저장
# 모델 저장 경로
DIR_SAVED_MODEL = 'C:/Users/User/Desktop/code/project/saved_model/final_model'
trainer.save_model(DIR_SAVED_MODEL)
tokenizer.save_pretrained(DIR_SAVED_MODEL)
```

### 3.3. Fine-Tuning한 모델 채점
```
# 저장된 모델 불러와서 테스트하기

from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
import torch
import pandas as pd

# 저장된 모델 불러오기
# 모델 저장 경로
DIR_SAVED_MODEL = 'C:/Users/User/Desktop/code/project/saved_model/final_model'
model = BertForSequenceClassification.from_pretrained(DIR_SAVED_MODEL) # 모델 불러오기
tokenizer = BertTokenizer.from_pretrained(DIR_SAVED_MODEL) # 토크나이저
model.eval()

# 다른 종목 텍스트 자료
DIR_TEST = 'C:/Users/User/Desktop/code/project/test_data_tsla_classified.csv'
df_tsla =pd.read_csv(DIR_TEST, encoding='latin1')  # comment, label

# 전체 데이터를 테스트로 사용
texts = df_tsla['body'].tolist()
true_labels = df_tsla['label'].tolist()

# 배치 처리 없이 한 번에 추론 (데이터가 적다고 가정)
inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=100)

# 추론
with torch.no_grad():
    outputs = model(**inputs)

# 예측값 추출
preds = torch.argmax(outputs.logits, dim=1).tolist()

# 결과 DataFrame 생성
df_result = pd.DataFrame({
    'comment': texts,
    'label': true_labels,
    'predicted_label': preds
})

# 결과 출력
print(df_result)
DIR_CHECK_ANOTHER = 'C:/Users/User/Desktop/code/project/check_tsla.csv'
df_result.to_csv(DIR_CHECK_ANOTHER)
```
\- 보다 정확도가 높은 모델을 선정하기 위한 추가 채점 과정  
\- 훈련 데이터에는 엔비디아(NVDA) 종목에 관한 글들의 댓글들을 사용했고, 추가 채점에는 테슬라(TSLA) 종목에 관한 글들의 댓글들을 사용했다.  


![image](https://github.com/user-attachments/assets/c1b29b3c-5e5a-4a57-ac0c-e09237639580)

\- 나같은 경우는 2개의 hyper parameter를 조정했다 : epoch 수, 데이터셋 수  
\- 총 24번 훈련시켰지만 fine-tuning, 추가 테스트 정답률의 최대 평균이 60%를 넘지 못했다. 더 많은 데이터셋을 가지고 훈련시켜야 할 듯  


### 3-4. 전체 서비스 코드
```
import streamlit as st
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime, timedelta
import pytz
import praw


##############################################
# 1. 레딧 코멘트 스크랩
##############################################

# Reddit API 인증
reddit = praw.Reddit(
    client_id='클라이언트 ID',
    client_secret='클라이언트 Secret',
    user_agent='유저 닉네임'
)

# 특정 티커에 대해 Reddit 코멘트를 수집하고, 최근 n일 이내의 것만 반환
def scrape_reddit_comments(ticker, subreddit_name='wallstreetbets', days=3, limit=1000):
    utc = pytz.UTC
    end_date = datetime.now(utc)
    start_date = end_date - timedelta(days=days)

    comments_data = []

    # Subreddit에서 인기 글 검색
    subreddit = reddit.subreddit(subreddit_name)
    for submission in subreddit.search(ticker, sort='new', time_filter='week', limit=limit):
        submission.comments.replace_more(limit=0)  # 더 많은 댓글 로딩
        for comment in submission.comments.list():
            comment_time = datetime.fromtimestamp(comment.created_utc, tz=utc)
            if start_date <= comment_time <= end_date:
                comments_data.append({
                    'comment_id': comment.id,
                    'created_utc': comment_time,
                    'body': comment.body,
                    'score': comment.score, # 코멘트 추천수
                    'submission_title': submission.title,
                    'permalink': f"https://reddit.com{comment.permalink}"
                })

    df = pd.DataFrame(comments_data) # 스크랩한 코멘트 담겨 있는 데이터프레임

    # date 컬럼 추가 : '년도-월-일' 형식
    df['date'] = pd.to_datetime(df['created_utc']) # created_utc를 datetime 객체로 변환
    df['date'] = df['date'].dt.tz_convert('Asia/Seoul') # UTC -> KST(Asia/Seoul) 서울로 시간대 변환
    df['date'] = df['date'].dt.date # '년도-월-일' 형식의 날짜 정보만 추출한 새 컬럼

    return df

##############################################
# 2. 긍정, 부정, 중립 분석 모델
##############################################

def load_model_and_tokenizer(model_path):
    model = BertForSequenceClassification.from_pretrained(model_path) # 저장된 모델 불러오기
    tokenizer = BertTokenizer.from_pretrained(model_path) # 저장된 토크나이저 불러오기
    return model, tokenizer

def predict_sentiments(model, tokenizer, df):
    model.eval() # 평가 모드로 전환
    sentiments = []
    with torch.no_grad():
        for text in df["body"].tolist():
            inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=100)
            outputs = model(**inputs)
            preds = torch.argmax(outputs.logits, dim=1).item()
            sentiments.append(preds)

    df["label"] = sentiments # 긍정, 부정, 중립 판별해서 라벨링하기
    label_map = {0: "부정", 1: "중립", 2: "긍정"}
    df["sentiment"] = df["label"].map(label_map) # 라벨링된 판별 해석
    return df

##############################################
# 3. 시각화
##############################################

def show_daily_sentiment_chart(df):
    daily = df.groupby(["date", "sentiment"]).size().unstack().fillna(0)
    st.line_chart(daily)

def show_cumulative_chart(df):
    cumulative = df["sentiment"].value_counts()
    st.bar_chart(cumulative)

def show_wordcloud(df):

    text = " ".join(df["body"])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    st.pyplot(plt)

def get_top_comment(df): # 추천수가 가장 많은 댓글
    top = df.sort_values(by="score", ascending=False).iloc[0]
    return top["body"]

# 기능 삭제
# def save_output_comment(df, ticker):
#     df.to_csv(f"outputs/{ticker}_result.csv", encoding = 'utf-8-sig', index=False)

#     with open(f"outputs/{ticker}_chatbot_response.txt", "w", encoding="utf-8") as f:
#         f.write("긍부정 결과 요약\n")
#         f.write(str(df["sentiment"].value_counts()))

##############################################
# 4. streamlit
##############################################

# 페이지 설정
st.set_page_config(page_title='종목 민심 살피기', layout='wide')
st.markdown("💬 **원하는 종목 티커와 기간을 입력하면 Reddit 코멘트를 기반으로 민심 분석을 해드려요!**")

# 상태 저장을 위한 세션 상태 초기화
if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_contexts" not in st.session_state:
    st.session_state.last_contexts = []
if 'ticker_history' not in st.session_state:
    st.session_state.ticker_history = []
if "latest_ticker" not in st.session_state:
    st.session_state.latest_ticker = "" # 페이지가 새로 렌더링 될 때마다 이전에 조사된 티커명 출력하기 위해

# 기존 대화 내용 출력
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# 이전 출력된 그래프 띄워놓기
if "last_comments_df" in st.session_state:
    with st.chat_message("assistant"):
        st.markdown(f"📅 {st.session_state.latest_ticker} 분석 결과 - 일자별 민심 비율")
        show_daily_sentiment_chart(st.session_state.last_comments_df)
        st.markdown(f"📊 {st.session_state.latest_ticker} 분석 결과 - 누적 민심 비율")
        show_cumulative_chart(st.session_state.last_comments_df)
        st.markdown(f"💡 {st.session_state.latest_ticker} WordCloud")
        show_wordcloud(st.session_state.last_comments_df)

# 채팅 입력창
user_input = st.chat_input("민심이 궁금한 티커를 검색해보세요. 검색 양식 : 티커, 기간 (NVDA, 3)")

if user_input:
    # 사용자 메시지 기록
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    
    try:
        ticker, days = str(user_input).strip().split(',') # 입력받은 값을 티커, 스크랩 기간으로 변경
    except:
        bot_response14 = "올바른 형태로 입력해주세요."
        with st.chat_message("assistant"):
            st.markdown(bot_response14)
        st.session_state.messages.append({"role": "assistant", "content": bot_response14})
    else:
        if ticker and days:
            st.session_state.latest_ticker = str(ticker).upper() # 페이지가 새로 렌더링 될 때마다 이전에 조사된 티커명 출력하기 위해
            st.session_state.ticker_history.append(ticker.upper()) # 입력된 티커 모두 대문자로 변환

            with st.spinner(f"{ticker.upper()}에 대한 코멘트 수집 중..."):
                comments_df = scrape_reddit_comments(ticker, days=int(days)) # 레딧 코멘트 스크랩
                comments_df.to_csv(f"data/{ticker.upper()}_comments.csv", index=False) # 스크랩된 코멘트들 저장

            bot_response1 = f"{ticker.upper()} 관련 총 {len(comments_df)}개의 코멘트를 수집했습니다." # 봇 답변1. 수집 완료 문구
            bot_response2 = f"{ticker.upper()} 의 민심을 분석 중입니다." # 봇 답변2. 감성 분석 중 문구 띄우기
            with st.chat_message("assistant"): # 수집 완료 문구, 감성 분석 중 문구 출력
                st.markdown(bot_response1)
                st.markdown(bot_response2)
            
            # 감성 분석
            DIR_MODEL = 'C:/Users/User/Desktop/code/project/saved_model/final_model' # 불러올 모델 경로
            model, tokenizer = load_model_and_tokenizer(DIR_MODEL) # 저장된 모델, 토크나이저 불러오기
            comments_df = predict_sentiments(model, tokenizer, comments_df) # 판별 결과 컬럼 추가하여 반환
            
            bot_response3 = f"{ticker.upper()} 의 민심이 분석 완료되었습니다. 잠시만 기다리십시오." # 봇 답변3. 감성 분석 완료 문구 띄우기

            with st.chat_message("assistant"): # 감성 분석 완료 문구 출력
                st.markdown(bot_response3)

            # 시각화 및 챗봇 답변 출력
            bot_response4 = f"📅 {ticker.upper()} 일자별 민심 비율" # 봇 답변4. 일자별 민심 비율 문구
            bot_response5 = f"📊 {ticker.upper()} 누적 민심 비율" # 봇 답변5. 누적 민심 비율 문구
            bot_response6 = "💡 단어 Wordcloud" # 봇 답변6. wordcloud 문구
            bot_response7 = "🏆 추천 수 가장 높은 코멘트" # 봇 답변7. 베스트 코멘트 문구
            bot_response8 = get_top_comment(comments_df) # 봇 답변8. 베스트 코멘트
            bot_response9 = "✅ 코멘트 자료 다운로드" # 봇 답변9. 코멘트 자료 다운로드 버튼
            bot_response10 = "🔍 분석이 끝났습니다! 더 궁금한 종목이 있으면 말씀해 주세요 😊" # 봇 답변10. 마무리 안내 메시지

            # 챗봇 대답
            with st.chat_message("assistant"): # 일자별, 누적 민심, 베스트 코멘트 출력
                st.markdown(bot_response4)
                bot_response11 = show_daily_sentiment_chart(comments_df) # 일자별 민석
                st.markdown(bot_response5)
                bot_response12 = show_cumulative_chart(comments_df) # 누적 민심 출력
                st.markdown(bot_response6)
                bot_response13 = show_wordcloud(comments_df) # wordcloud 출력
                st.markdown(bot_response7)
                st.markdown(bot_response8)
                st.markdown(bot_response9)

            st.session_state.last_comments_df = comments_df # 새로운 텍스트 입력 후에도 그래프 사라지지 않게 하기 위해

            csv = comments_df.to_csv(index=False).encode('utf-8-sig') # 다운로드 위해 csv로 변환
            st.download_button(
                label = "코멘트 파일 다운로드",
                data = csv,
                file_name = f"{ticker}_comments.csv",
                mime="text/csv"
            )
```

# 4. 결과물

![image](https://github.com/user-attachments/assets/dc76b7c6-7f09-433d-97d9-930f4a403c31)

![image](https://github.com/user-attachments/assets/9edad9e5-df17-4dcb-ade7-f29125d37683)

![image](https://github.com/user-attachments/assets/8425a454-b05a-42e5-b19f-3c43f316bd54)

![image](https://github.com/user-attachments/assets/1d9c7fe3-b01c-4369-b8cb-b739d064358d)





